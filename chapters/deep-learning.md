## :globe_with_meridians: ディープラーニング

ディープニューラルネットワークを用いた機械学習の手法。  
2010年代に脚光を浴びたが、アルゴリズム自体は1960年代にはすでに考案されていた。  

- ニューロン  
単純な数値予測ができ予測器。ニューラルネットワークの最小単位  
重みが乗算された入力を受け取り、それらを総和して出力する

- ニューラルネットワーク  
ニューロンをたくさんつなげててきる予測器。入力が行わる層を「入力層」、出力される層を「出力層」、それ以外の層を「中間層」または「隠れ層」と呼ぶ。  
人間の脳を模倣したモデルではなく、「人間の脳の仕組みの一部」を模倣している。

- [ノーフリーランチ定理](ノーフリーランチ定理)  
「あらゆる問題に対して万能なアルゴリズムは存在しない」という定理

- [ネオコグニトロン](https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%AA%E3%82%B3%E3%82%B0%E3%83%8B%E3%83%88%E3%83%AD%E3%83%B3)  
1982年に福島邦彦氏によって発表された畳み込みニューラルネットワークの発想の基となったネットワークモデル

- 単純パーセプトロン

- 多層パーセプトロン

- オートエンコーダ  
砂時計型の構造をもつニューラルネットワークで、入力側から半分をエンコーダ、残り半分をデコーダーと呼ぶ。  
入力層と出力層のノード数が等しく、中間層のノード数が入出力層よりが少ない。  
入力と全く同じ藻をを出力することを目指して学習する。（「正解ラベル」として「入力自身」を用いる）

- 次元削減  
オートエンコーダにおいて、隠れ層で特徴的な情報だけに次元を圧縮すること

- ディープオートエンコーダ  

- 事前学習  

- ファインチューニング  


### 活性化関数

入力信号の総和を出力信号に変換する関数。  
出力層で使用する活性化関数は、回帰問題では恒等関数、分類問題ではソフトマックス関数を一般的に利用する。

パーセプトロンとニューラルネットワークの主な違いは、活性化関数。  
パーセプトロンではステップ関数、ニューラルネットワークではシグモイド関数など非線形関数を用いる。

- ステップ関数  
微分できないのでニューラルネットワークの学習で実際に使われることはない

- [シグモイド関数](https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0)  
入力を 0 ~ 1の間に値に変換する性質を持つ関数。  
ステップ関数と形が似ていて、なめらかなので微分できる関数として考案された

- [tanh (ハイパボリックタンジェント関数)](https://ja.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E7%B7%9A%E9%96%A2%E6%95%B0)  

- [ReLU関数](https://ja.wikipedia.org/wiki/%E6%AD%A3%E8%A6%8F%E5%8C%96%E7%B7%9A%E5%BD%A2%E9%96%A2%E6%95%B0)  
正則化機能を持たいない活性化関数で、勾配消失問題が起きにくく、最近の主流。  
入力が 0 を超えていればその入力をそのまま出力し、0 以下ならば 0 を出力する。  

- ソフトマックス関数  
出力を正規化して、確率として解釈する際に用いされる活性化関数。
分類問題の出力層付近で用いられることが一般的。

- Leaky ReLU関数  

- Parametric ReLU  

- Randomized ReLU  

### 学習の流れ
1. 教師データを用いて予測計算をする。その際の予測値を正解ラベルと比較して誤差を計算する。
1. 予測値は左から右へと順番に伝わる（順伝播）
1. 上記をいくつかの教師データについて繰り返し、誤差を足し合わせる
1. 累計された誤差が小さくなるように、勾配降下法を用いて各枝の重みを更新する。  
   枝の重みは右から左へと順番に更新される（誤差逆伝伝播法）
1. 上記を繰り返す

### 学習率の最適化
- [勾配降下法](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)  
重みを少しずつ更新して勾配が最小になる点を探索するアルゴリズム。  
ディープニューラルネットワークの学習では、「局所最適解が求められればそれなりに誤差の値は小さくなるだろうから、それで妥協しよう」というスタンスに立つ。

  - 局所最適解  
    園周辺では誤差の値は小さいが、最小値を実現するわけではない解
  - 大局的最適解  
    誤差の値を最も小さくする解
  - 停留点  
    局所最適解でも大局的最適解でもないが、勾配が 0 になる点
  - 鞍点（あんてん）  
    停留点のうち、ある方向から見ると極小値だが、別の方向から見ると極大値になる点

- [誤差関数](https://ja.wikipedia.org/wiki/%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0)  

- エポック  
訓練データを何度学習に用いたか

- イテレーション  
  重みを何度更新したか

- 確率的勾配降下法  
訓練データ1つに対して、重みを1回更新する（データ１つごとにイテレーションが増える）

- ミニバッチ勾配降下法  
ミニバッチに含まれるすべてのデータについて誤差の総和を計算し、その総和を小さくするように重みを1回更新する。  
ミニバッチは、いくつかの訓練データからランダムにサンプリングした小さなデータの集まり。

- 勾配降下法  
訓練データすべての誤差を経緯さん氏、重みを1回更新する（イテレーションとエポックが等しい）

- Adagrad  
勾配降下法の学習率に関する手法。

- 蒸留  
大きいネットワークの入出力を小さいネットワークに再学習される手法

- Adadelta  

- RMSprop  

- Adan  

- 勾配消失問題  
誤差の勾配を逆伝播する過程において、勾配の値が消出し入力層付近での学習が進まなくなるディープニューラルネットワーク特有の現象。層が深いほど起こりやすい。  
活性化関数が何度も作用することで勾配が小さくなりすぎてしまうことが原因とされる。


### テクニック
- ドロップアウト  
重み更新の際に一定の割合でランダムに枝を無効化する手法。単純だが効果が高い。

- early stopping  

- 正規化  

- 標準化  

- 白色化  

- バッチ正規化  


### CNN: 畳み込みニューラルネットワーク

特に画像認識に応答するために改良されたディープニューラルネットワーク。  
自動運転技術など、非常に広く応用されている。

#### 処理の流れ

1. 畳み込み層  
フィルタを用いて積和演算 + 活性化関数の作用を行う層。  
元の画像の特徴が抽出された小さな画像である「特徴マップ」に変換される。

2. プーリング層  
畳み込み層から「特徴マップ」を受け取り、平均値、最大値を用いてサブサンプリングを行う層。  
平均プーリング、最大プーリングによって更に小さな画像に変換される。

3. 全結合層  
プーリング層から出力された画像データを、縦横に並んだ2次元データから、一列に並べたフラットな1次元データに変換する出力層

#### 手法

- LeNet  

- 畳み込み  

- プーリング  

- maxプーリング  

- avgプーリング  

- 全結合層  

- データ拡張  

- AlexNet  

- VGG  

- GoogLeNet  

- Skip connection  
層を飛び越えた結合

- 転移学習  
学習済みのネットワークを利用して新しいタスクの識別に利用すること

### RNN: リカレントニューラルネットワーク

時系列データを入力して、データから時間依存性を学習できるモデル。  
内部に閉路（行って戻ってくる経路）を持つニューラルネットワーク。  
RNN は過去の情報を保持できるため、過去の入力を参考に「時系列データの次の時点での値」を予測する。  
自然言語処理への応用が盛んで、機械翻訳技術などに応用されている。

- 入力重み衝突  

- 出力重み衝突  

- BPTT (Backpropagation Through Time)  
RNN を順伝播型ネットワークに置き換えて誤差逆伝播法を適用する手法

- CTC  
入出力間で系列長が違う場合のニューラルネットワークを用いた分類法

- LSTM (Long Short-Term Memory)  
遠い過去の入力を現在の出力に反映する手法

- GRU (Gated Recurrent Unit)  

- Bidirectional RNN  
LSTM を2つ組み合わせることで、未来から過去方向も含めて学習できるモデル

- Sequence-to-Sequence  
入力が時系列なら出力も時系列で予測する。自然言語処理を中心に研究されている分野。

- RNN Encoder–Decoder

- Attention  
過去の時点それぞれの重みを学習することで、時間の重みをネットワークに組み込む。

### 深層強化学習

- 強化学習  
行動を学習する仕組み。目的とする報酬（スコア）を最大化するためにはどのような行動をとっていけばいいかを学習していく。

### 深層生成モデル

- WaveNet  

- 生成モデル  

- VAE (変分オートエンコーダ)  

- GAN (敵対的生成ネットワーク)  
2014年にイアン・グッドフェローらによって発表された、生成ネットワークと識別ネットワークからなる教師なし学習法。

  - 生成ネットワーク  
    訓練データと同じようなデータを生成する

  - 識別ネットワーク  
    そのデータが訓練データから来たものか、生成ネットワークから来たものかを識別する

- DCGAN (Deep Convolutional GAN)  
