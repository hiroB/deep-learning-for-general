## :globe_with_meridians: ディープラーニング

ディープニューラルネットワークを用いた機械学習の手法。  
2010年代に脚光を浴びたが、アルゴリズム自体は1960年代にはすでに考案されていた。  

深い関数を使った最小二乗法。  
シグモイド関数など活性化関数を使って非線形性を入れ、多層に構成した関数を使った、損失関数の最小化。

### ニューラルネットワーク

- ニューロン  
単純な数値予測ができ予測器。ニューラルネットワークの最小単位  
重みが乗算された入力を受け取り、それらを総和して出力する

- ニューラルネットワーク  
ニューロンをたくさんつなげててきる予測器。入力が行わる層を「入力層」、出力される層を「出力層」、それ以外の層を「中間層」または「隠れ層」と呼ぶ。  
人間の脳を模倣したモデルではなく、「人間の脳の仕組みの一部」を模倣している。

- [ネオコグニトロン](https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%AA%E3%82%B3%E3%82%B0%E3%83%8B%E3%83%88%E3%83%AD%E3%83%B3)  
1982年に福島邦彦氏によって発表された畳み込みニューラルネットワークの発想の基となったネットワークモデル

- 宝くじ仮説
「ランダムに初期化された密なニューラルネットワークは、たまたま、うまく学習ができるように初期化されたサブネットワークが含まれており、このサブネットは、学習が進むにつて、他のサブネットよりも優れているので早く学習が進み、他の劣るサブネットの活性が抑制される」という仮説。  
この仮説を提唱した論文「[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)」は、2019年5月にはディープラーニングのカンファレンス ICLR 2019で [Best Paper Award](https://iclr.cc/Conferences/2019/Awards) に選ばれた。

### 従来の機機械学習との違い

機械学習では、対象領域の知識に基づいて適切に手法を選択する必要があるが、ディーブラーニングでは、表現力の高い「深い関数」を用いるため、データを計算量されあれば精度が上がる。  
ディーブラーニングに適した学習データは、正規化されているものよりも、画像や音声そのもの、膨大なテキストなど。  
どうやてデータを集めるか、どうやってアノテーション（ラベルやメダデータを与えて学習データを整備）するかが課題となる。

### 用語

- [ノーフリーランチ定理](ノーフリーランチ定理)  
「あらゆる問題に対して万能なアルゴリズムは存在しない」という定理

- 単純パーセプトロン

- 多層パーセプトロン

- オートエンコーダ  
2006年に発表され、今日のディーブラーニング隆盛のきっかけともなった技術。  
次元数を減らしたニューロン層を重ねていくことで特徴の圧縮を行うエンコーダと逆の構造を持つデコーダーを接続したもの。  
砂時計型の構造をもつニューラルネットワークで、入力側から半分をエンコーダ、残り半分をデコーダーと呼ぶ。  
入力層と出力層のノード数が等しく、中間層のノード数が入出力層よりが少ない。  
入力情報と同じ出力情報を再現するこを目指す（「正解ラベル」として「入力自身」を用いる）

- 次元削減  
オートエンコーダにおいて、隠れ層で特徴的な情報だけに次元を圧縮すること

- ディープオートエンコーダ  

- 事前学習  

- ファインチューニング  


### 活性化関数

入力信号の総和を出力信号に変換する関数。  
出力層で使用する活性化関数は、回帰問題では恒等関数、分類問題ではソフトマックス関数を一般的に利用する。

パーセプトロンとニューラルネットワークの主な違いは、活性化関数。  
パーセプトロンではステップ関数、ニューラルネットワークではシグモイド関数など非線形関数を用いる。

- ステップ関数  
微分できないのでニューラルネットワークの学習で実際に使われることはない

- [シグモイド関数](https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89%E9%96%A2%E6%95%B0)  
入力を 0 ~ 1の間に値に変換する性質を持つ関数。  
ステップ関数と形が似ていて、なめらかなので微分できる関数として考案された

- [tanh (ハイパボリックタンジェント関数)](https://ja.wikipedia.org/wiki/%E5%8F%8C%E6%9B%B2%E7%B7%9A%E9%96%A2%E6%95%B0)  

- [ReLU関数](https://ja.wikipedia.org/wiki/%E6%AD%A3%E8%A6%8F%E5%8C%96%E7%B7%9A%E5%BD%A2%E9%96%A2%E6%95%B0)  
正則化機能を持たいない活性化関数で、勾配消失問題が起きにくく、最近の主流。  
入力が 0 を超えていればその入力をそのまま出力し、0 以下ならば 0 を出力する。  

- ソフトマックス関数  
出力を正規化して、確率として解釈する際に用いされる活性化関数。
分類問題の出力層付近で用いられることが一般的。

- Leaky ReLU関数  

- Parametric ReLU  

- Randomized ReLU  

### 学習の流れ
1. 教師データを用いて予測計算をする。その際の予測値を正解ラベルと比較して誤差を計算する。
1. 予測値は左から右へと順番に伝わる（順伝播）
1. 上記をいくつかの教師データについて繰り返し、誤差を足し合わせる
1. 累計された誤差が小さくなるように、勾配降下法を用いて各枝の重みを更新する。  
   枝の重みは右から左へと順番に更新される（誤差逆伝伝播法）
1. 上記を繰り返す

### 学習率の最適化
- [勾配降下法](https://ja.wikipedia.org/wiki/%E7%A2%BA%E7%8E%87%E7%9A%84%E5%8B%BE%E9%85%8D%E9%99%8D%E4%B8%8B%E6%B3%95)  
重みを少しずつ更新して勾配が最小になる点を探索するアルゴリズム。  
ディープニューラルネットワークの学習では、「局所最適解が求められればそれなりに誤差の値は小さくなるだろうから、それで妥協しよう」というスタンスに立つ。

  - 局所最適解  
    園周辺では誤差の値は小さいが、最小値を実現するわけではない解
  - 大局的最適解  
    誤差の値を最も小さくする解
  - 停留点  
    局所最適解でも大局的最適解でもないが、勾配が 0 になる点
  - 鞍点（あんてん）  
    停留点のうち、ある方向から見ると極小値だが、別の方向から見ると極大値になる点

- [誤差関数](https://ja.wikipedia.org/wiki/%E8%AA%A4%E5%B7%AE%E9%96%A2%E6%95%B0)  

- エポック  
訓練データを何度学習に用いたか

- イテレーション  
  重みを何度更新したか

- 確率的勾配降下法  
訓練データ1つに対して、重みを1回更新する（データ１つごとにイテレーションが増える）

- ミニバッチ勾配降下法  
ミニバッチに含まれるすべてのデータについて誤差の総和を計算し、その総和を小さくするように重みを1回更新する。  
ミニバッチは、いくつかの訓練データからランダムにサンプリングした小さなデータの集まり。

- 勾配降下法  
訓練データすべての誤差を経緯さん氏、重みを1回更新する（イテレーションとエポックが等しい）

- Adagrad  
勾配降下法の学習率に関する手法。

- 蒸留  
大きいネットワークの入出力を小さいネットワークに再学習される手法

- Adadelta  

- RMSprop  

- Adan  

- 勾配消失問題  
誤差の勾配を逆伝播する過程において、勾配の値が消出し入力層付近での学習が進まなくなるディープニューラルネットワーク特有の現象。層が深いほど起こりやすい。  
活性化関数が何度も作用することで勾配が小さくなりすぎてしまうことが原因とされる。


### テクニック
- ドロップアウト  
重み更新の際に一定の割合でランダムに枝を無効化する手法。単純だが効果が高い。

- early stopping  

- 正規化  

- 標準化  

- 白色化  

- バッチ正規化  


### CNN: 畳み込みニューラルネットワーク

特に画像認識に応答するために改良されたディープニューラルネットワーク。  
自動運転技術など、非常に広く応用されている。

従来のシグモイド関数のような活性化関数では、層が深くなるにつれ勾配が小さくなってくのでうまく学習ができないので、勾配消失問題を解決した ReLU関数を用いる。

#### 処理の流れ

1. 畳み込み層  
フィルタを用いて積和演算 + 活性化関数の作用を行う層。  
元の画像の特徴が抽出された小さな画像である「特徴マップ」に変換される。

2. プーリング層  
畳み込み層から「特徴マップ」を受け取り、平均値、最大値を用いてサブサンプリングを行う層。  
平均プーリング、最大プーリングによって更に小さな画像に変換される。

3. 全結合層  
プーリング層から出力された画像データを、縦横に並んだ2次元データから、一列に並べたフラットな1次元データに変換する出力層

#### 手法

- LeNet  

- 畳み込み  

- プーリング  

- maxプーリング  

- avgプーリング  

- 全結合層  

- データ拡張  

- AlexNet  

- VGG  

- GoogLeNet  

- Skip connection  
層を飛び越えた結合

- 転移学習  
学習済みのネットワークを利用して新しいタスクの識別に利用すること

### RNN: リカレントニューラルネットワーク

時系列データにおいて、ある時間的に近接した要素同士は影響を与え合う可能性が高いが、時間的に遠く離れた要素が影響を与えることは少ない。  
この性質を使えば、パラメータの数を減らすことができる。これが RNN である。

時系列データを入力して、データから時間依存性を学習できるモデル。  
内部に閉路（行って戻ってくる経路）を持つニューラルネットワーク。  
RNN は過去の情報を保持できるため、過去の入力を参考に「時系列データの次の時点での値」を予測する。  
自然言語処理への応用が盛んで、機械翻訳技術などに応用されている。

- 入力重み衝突  

- 出力重み衝突  

- BPTT (Backpropagation Through Time)  
RNN を順伝播型ネットワークに置き換えて誤差逆伝播法を適用する手法

- CTC  
入出力間で系列長が違う場合のニューラルネットワークを用いた分類法

- LSTM (Long Short-Term Memory)  
遠い過去の入力を現在の出力に反映する手法

- seq2seq  
時系列データを入力・処理し、時系列データを出力するモデル。  
代表的な応用として入力と出力を異なる言語列とする翻訳が実現できる。  
これは NMT（ニューラル機械翻訳）と呼ばれ、従来の SMT（統計的機械翻訳）よりも大幅に性能が向上している。

- GRU (Gated Recurrent Unit)  

- Bidirectional RNN  
LSTM を2つ組み合わせることで、未来から過去方向も含めて学習できるモデル

- Sequence-to-Sequence  
入力が時系列なら出力も時系列で予測する。自然言語処理を中心に研究されている分野。

- RNN Encoder–Decoder

- Attention  
過去の時点それぞれの重みを学習することで、時間の重みをネットワークに組み込む。

### 深層強化学習

システムが「状態」を定義し、試行錯誤を重ねながら、自動的に報酬を最大化する「方策」を見つけることができる。  
さらに探索による先読みを組み合わせることで、AlphaGo（碁）の躍進につながった。

- 強化学習  
行動を学習する仕組み。目的とする報酬（スコア）を最大化するためにはどのような行動をとっていけばいいかを学習していく。  
機械学習では、想定する「状態」の数が非常に多くなったり、人による「状態」の想定に限界があったりした。

- [BRETT](https://engineering.berkeley.edu/brett/)  
カリフォルニア大学バークレー後が開発している、ディーブラーニングと強化学習を組み合わせたロボット。  
報酬の設定を変更すれば、異なる動作を学習することができる。  
一旦学習した内容はコピーして、同じタイプのロボットであれば同じように動かすことができる。

- Q学習

- [DQN (Deep Q-Network)](https://ja.wikipedia.org/wiki/DQN_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF))  
DeepMind が考案した、Q学習の行動価値関数を、深い構造を持ったニューラルネットワークで置き換えたモデル。  
アタリ社のゲームを学習された例では、49ゲームのうち29ゲームで、人間並、あるいはそれ以上のスコアを出した。

### 深層生成モデル

ディーブラーニング技術は、識別や回帰だけでなく、これまで存在していなかったデータ、例えば架空の画像の生成にも利用できる。  
一般に「生成モデル」と呼ばれるモデルを使うことで、AI 技術による「創作」が可能となる。  
ディープニューラルネットワークを使った生成モデルは「深層生成モデル」と呼ばれる。

- WaveNet  

- 生成モデル  

- VAE (変分オートエンコーダ)  
オートエンコーダの中間層の圧縮された特徴表現の数値を確率分布に従うように変更し、さらにエンコーダの出力とデコーダーの入力をこれにあわせて変更したのもの。  
確率分布に従うことで、圧縮された特徴表現の数値を変動させ、デコーダーの所期の性能（いろいろな猫の画像の生成）を達成できる。

- GAN (敵対的生成ネットワーク)  
2014年にイアン・グッドフェローらによって発表された、生成ネットワークと識別ネットワークからなる教師なし学習法。  
トレーニングに利用したデータに類似した出力を生成できるため、希少データの水増しや、作風をまねた絵画や楽曲の生成など、様々な応用が進められている。

  - 生成ネットワーク  
    ランダムノイズベクトルから入力データのレプリカを作ろうとする
    訓練データと同じようなデータを生成する

  - 識別ネットワーク  
    そのデータが訓練データから来たものか、生成ネットワークから来たものかを識別する

- DCGAN (Deep Convolutional GAN)  

### GNN: グラフニューラルネットワーク

グラフ構造（データ間のつながりを持った構造）のデータを入力とするニューラルネットワーク。  
GNN やオートエンコーダ、RNN を構築することができるため、広範囲の応用が可能。  
分子構造の予測や自然言語処理などへの応用がある。
