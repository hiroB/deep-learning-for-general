## :robot: 機械学習

機械学習はデータが命。データから答えを探し、データからパターンを見つけ、データからストーリーを語る。  
機械学習の中心には「データ」があり、このデータ駆動によるアプローチは、「人」を中心とするアプローチからの脱却とも言える。  
重みパラメータの値をデータから自動で計算できる。（学習する）

### 代表的な手法

- 教師あり学習  
教師データ（入力とそれに対応する正解ラベルの組）を使って予測値を正解ラベルに近づけることを目標に学習する手法

- 教師なし学習  
教師データを使わずに、データの本質的な構造を浮かび上がらせる手法

- 強化学習  
収益（報酬の和）を最大化する方策を獲得することを目的とした手法

- 半教師あり学習  
教師あり学習におけるラベル付けコストを低減するために用いられる手法。
データの一部分にのみ正解ラベルをつける。

- 回帰問題  
出力値の予測（ex. 家賃、年収、天気の予測）

- 線形回帰  
入力データが1つの単回帰分析、複数の入力データの重回帰分析などの種類がある

- 分類問題  
あらかじめ設定したクラスにデータを割り振る（ex. 画像に写っている犬や猫の識別）。  
サポートベクターマシン、決定木、ランダムフォレスト、ロジスティック回帰、kNN法などの手法がある。深層学習でも分類を行うことができる。

### データの前処理

データをモデルに正しく入力できるようにする。  
データの大きさをある程度均一にする。

- 正規化  
データを 0 から 1 に収まるようにスケーリングすること

- 標準化  
平均を 0、標準偏差（分散）を 1 に変換すること

- 正則化  
過学習を抑制するための手法。  
誤差関数にパラメータのノルムによる正則化（LASSOなど）を付け加える

- 基礎集計  
データの傾向を事前に把握する。散布図行列をプロットして傾向を調べる。相関行列を表示し傾向を調べる。


### 教師あり学習

- 線形回帰  
  - 単回帰分析  
  1つの説明変数（手がかりとなる変数）から目的変数を予測する  
  ※ 説明変数は文脈にとっては特徴量と呼ばれることがある。特徴量の方が少し大きな概念になるが、同じものを指していると考えていい。

  - 重回帰分析  
  複数の説明変数から目的変数を予測する

- 相関係数  
特徴量同士の相関の正負と強さを表す指標。-1 <= x <= 1  
1に近いほど強い正の相関、-1に近いほど強い負の相関をもつ。0のときは相関がない。

- 多重共線性  
相関係数が高い特徴量の組を同時に説明変数に選ぶと、予測がうまくいかなくなる現象。  
特徴量エンジニアリングにおいては、多重共線性がでないような特徴量を選ぶ必要がある。

- 線形分離可能  
パーセプトロンを使って解ける問題。2次元のグラフ上で直線を使って分離できる。  
論理ゲートでは OR, AND, NAND は線形分離可能だが、XOR のみ線形分離不可能。

- [SVM (サポートベクターマシン)](https://ja.wikipedia.org/wiki/%E3%82%B5%E3%83%9D%E3%83%BC%E3%83%88%E3%83%99%E3%82%AF%E3%82%BF%E3%83%BC%E3%83%9E%E3%82%B7%E3%83%B3)  
「マージン最大化」というコンセプトで2つのクラスを線形分離可能にするアルゴリズム。  
もともとは2つのクラス分類アルゴリズムとして考案されたが、性能がいいので多クラス分類や回帰分析にも応用されている。  
線形分離可能でないデータに対しても、カーネル法を組み合わせることで決定境界を求めることができる。

- [スラック変数](http://sudillap.hatenablog.com/entry/2013/04/08/235602)  
超平面を構成した結果として発生する誤差の程度を測る変数。


- [ハイパーパラメータ](https://ja.wikipedia.org/wiki/%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF)  
推論や予測の枠組みの中で決定されないパラメータ。  
SVM においては誤りをどの程度許容するかの度合いをエンジニアが事前に調整する必要がある。

- [カーネル法](https://ja.wikipedia.org/wiki/%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E6%B3%95)  
次のアイディアをカーネル関数の計算によって実現するもの  
・ データを高次元空間にうまく埋め込む  
・高次元空間で線形分離 (SVM) し、その境界を元の空間に戻す

- カーネルトリック  
カーネル関数を使って、計算複雑度の増大を抑えつつ内積にもとづく解析手法を高次元特徴空間へ拡張するアプローチ。  
計算量を現実的に抑えつつ、非線形分離を実現する。

- 決定木  
以下のような仕組みの分類アルゴリズム。  
  - 不純度が最も減少（情報利得が最も増加）するように、条件分岐を作りデータを振り分ける  
  - それを繰り返す  

  不純度とは「クラスの混じり具合」を表す指標で、代表的なものにジニ係数やエントロピーがある。

- ランダムフォレスト  
決定木にバギングを組み合わせた手法。以下のような利点があり、非常によく使われている。  
  - データの前処理が少なくて済む  
  - 安定して良い精度が出る

- バギング  
アンサンブル学習の一種で、複数のモデルを使用し、それらの平均の多数決をとって予測結果とする

- [ロジスティク回帰](https://ja.wikipedia.org/wiki/%E3%83%AD%E3%82%B8%E3%82%B9%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E5%9B%9E%E5%B8%B0)  
線形回帰を分類問題に応用したアルゴリズム。  
  1. 「対数オッズ」を重回帰分析により予測する  
  2. 「対数オッズ」をロジスティック関数（シグモイド関数）で変換することで、クラスiに属する確率piを求める  
  3. 各クラスに属する確率を計算し、最大確率を実現するクラスが、データの所属するクラスと予測する  
また、「ロジット変換」を行うことで、出力値が 0 から 1 の間の値に正規化され、確率としての解釈が可能になる

- [kNN法 (k-nearest neighbor)](https://ja.wikipedia.org/wiki/K%E8%BF%91%E5%82%8D%E6%B3%95)  
クラス分類のアルゴリズムの一種。   
データから近い順に k 個のデータを見て、それらの多数決によって所属クラスを決定する。  
kNN法は実用上、以下の条件がないと精度が上がらない弱点がある。  
  - 各クラスのデータ数に偏りがない  
  - 各クラスがよく別れている  

- ブートストラップサンプリング  

- ブースティング
  - AdaBoost
  - 勾配ブースティング
  - XgBoost

- ニューラルネットワーク
  - 単純パーセプトロン
  - 誤差逆伝播法
  - シグモイド関数

### 教師なし学習

代表的な手法は、クラスタリングと次元分析。

- クラスタリング  
データ群をいくつかの集まり（クラスタ）に分けることで、データの本質的な構造を浮かび上がらせる手法。  
クラスタはデータから自動的に導かれる。

- クラス分類  
エンジニアが予め設定した「クラス」にデータを適切に分類する（教師データを使う）

- 次元圧縮  
データの情報を失わないようにデータを低い次元に圧縮する手法の総称。（ex. 身長と体重から肥満度を表す BMI を計算する）

- 主成分分析  
データから「主成分」という新たなデータを求める、線形な次元削減の手法。

- [k-means (k平均法)](https://www.albert2005.co.jp/knowledge/data_mining/cluster/non-hierarchical_clustering)  
異なる性質のものが混ざり合った集団から、互いに似た性質を持つものを集め、クラスターを作る方法の1つ。  
あらかじめいくつのクラスターに分けるかを決め、決めた数の塊（排他的部分集合）にサンプルを分割する。

### 手法の評価

- 訓練データ  
学習に用いる分の教師データ

- テストデータ  
未知データへの予測性能（汎化性能）を測るための教師データ

- 過学習  
学習済みのモデルが、訓練に対しては高い精度で正解ラベルを予測できる（訓練誤差が小さい）にもかかわらず、未知のデータに対しての予測精度が悪いままになってしまう現象。  
機械学習における最大の問題？と呼ばれている。

- ホールドアウト検証  
データを訓練データとテストデータにわけて、モデルが過学習を起こしていないか調べるための手法。

- モデルを評価するためのデータ
  - テストデータ  
やがて手に入るであろう、正解ラベルがあるとは限らないデータ

  - 検証データ  
あらかじめ手元にあり、正解ラベルがあることが約束されているデータ

- [交差検証](https://ja.wikipedia.org/wiki/%E4%BA%A4%E5%B7%AE%E6%A4%9C%E8%A8%BC)  
テストデータに用いるブロックを順に移動しながらホールドアウト法による検証を行う。  
計算量が多くなるなど欠点があるが、データ数が少ない場合にもホールドアウト法と比較して信頼できる精度が偉えるなどの理由で、精度検証において最もよく用いられる。

- k-分割交差検証  

- 混同行列  

- 正解率  

- 適合率  

- 再現率  

- F値  

- オーバーフィッティング  

- アンダーフィッティング  

- ラッソ回帰  

- リッジ回帰  

- Elastic Net  
